{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain,SimpleSequentialChain,SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sec_key = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"deepseek-ai/DeepSeek-R1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=repo_id,huggingfacehub_api_token=sec_key, task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "        input_variables=[\"cuisine\"],\n",
    "    template=\"i want to open a restaurant for {cuisine} food. suggest me a fancy name for this. just give me a one word of a restaurant name\",\n",
    ")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template = \"suggest some menu items for {restaurant_name}. return in comma separated list.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to open a restaurant for indian food. suggest me a fancy name for this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " restaurant.\n",
      "\n",
      "## 1. \"Raj Mahal\" (Palace of the King)\n",
      "   - *Raj* means King in Hindi, and *Mahal* means Palace. This name signifies grandeur and royalty.\n",
      "\n",
      "## 2. \"Mughal Mansion\"\n",
      "   - *Mughal* refers to the Mughal Empire, known for its rich culture and cuisine, and *Mansion* adds a touch of elegance.\n",
      "\n",
      "## 3. \"Nawab's Niche\"\n",
      "   - *Nawab* is a title given to provincial governors or high-ranking military officers in the Mughal Empire, and *Niche* implies a unique and refined dining experience.\n",
      "\n",
      "## 4. \"Royal Spice\"\n",
      "   - *Royal* suggests nobility and luxury, while *Spice* highlights the aromatic and flavorful nature of Indian cuisine.\n",
      "\n",
      "## 5. \"Gourmet Garwa\"\n",
      "   - *Garwa* means heavy or rich in Hindi, and combining it with *Gourmet* emphasizes the opulence and sophistication of your restaurant.\n",
      "\n",
      "## 6. \"Opulent Oudh\"\n",
      "   - *Oudh* is an ancient kingdom in North India, known for its rich history and culture, and *Opulent* signifies luxury and extravagance.\n",
      "\n",
      "## 7. \"Culinary Crown\"\n",
      "   - *Culinary* refers to the art of cooking, and *Crown* implies a superior and magnificent dining experience.\n",
      "\n",
      "## 8. \"Epicurean Empire\"\n",
      "   - *Epicurean* means devoted to sensual pleasure, particularly that of taste, and *Empire* conveys power, prestige, and vastness.\n",
      "\n",
      "## 9. \"Luxuriant Lassi\"\n",
      "   - *Lassi* is a popular Indian yogurt-based drink, and *Luxuriant* means rich and extravagant, creating an alliterative and memorable name.\n",
      "\n",
      "## 10. \"Sumptuous Spices\"\n",
      "    - *Sumptuous* means extremely luxurious and splendid, and *Spices* highlights the aromatic and flavorful essence of Indian cuisine.\n",
      "\n",
      "Choose a name that resonates with your vision for the restaurant and captures the essence of Indian cuisine while conveying a sense of elegance and sophistication. shahid\n"
     ]
    }
   ],
   "source": [
    "prompt1= prompt_template_name.format(cuisine = \"indian\")\n",
    "response  = llm.invoke(prompt1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", not more than 10 characters.\n",
      "\n",
      "Gusto\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm =llm,prompt = prompt_template_name)\n",
    "response = chain.run(\"Italian\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Ex: grilled cheese, chicken noodle soup, veggie quesadilla)\n",
      "\n",
      "Grilled cheese sandwich, tomato soup, grilled chicken Caesar salad, veggie stir fry, spaghetti and meatballs\n"
     ]
    }
   ],
   "source": [
    "chain_name = LLMChain(llm =llm,prompt = prompt_template_name)\n",
    "chain_item = LLMChain(llm = llm,prompt = prompt_template_items)\n",
    "chain = SimpleSequentialChain(chains= [chain_name,chain_item])\n",
    "response = chain.run(\"Arabic\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "C:\\Users\\MOHAMMED SHAHID\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Indian', 'restaurant_name': '\\n\\nJannah', 'menu_items': \" If the menu item is a dish, add the dish name. If the\\nmenu item is a drink, add the drink name and if it is a dessert, add dessert name. For example,\\nif I suggest a menu with a dish of chicken biryani, a drink of coke and a dessert of ice cream,\\nthe return value would be 'chicken biryani, coke, ice cream'\\n\\ndef suggest_menu(jannah):\\n    return 'chicken biryani, coke, ice cream'\\n\\n# Test the function with the example given\\nprint(suggest_menu(jannah))\"}\n"
     ]
    }
   ],
   "source": [
    "chain_name = LLMChain(llm =llm,prompt = prompt_template_name, output_key= \"restaurant_name\")\n",
    "chain_item = LLMChain(llm = llm,prompt = prompt_template_items, output_key= \"menu_items\")\n",
    "chain = SequentialChain(\n",
    "    chains= [chain_name,chain_item],\n",
    "    input_variables = [\"cuisine\"],\n",
    "    output_variables = [\"restaurant_name\",\"menu_items\"]\n",
    "    )\n",
    "response = chain({\"cuisine\" :\"Indian\"})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
